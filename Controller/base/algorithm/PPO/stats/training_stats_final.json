{
  "training_rewards": [
    -10,
    -9.586338998124983,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10,
    -10
  ],
  "episode_lengths": [
    1,
    2,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1
  ],
  "actor_losses": [],
  "critic_losses": [],
  "entropy_losses": []
}