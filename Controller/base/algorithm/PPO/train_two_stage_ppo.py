#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import torch
import numpy as np
import os
import json
import time
from tqdm import tqdm
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt

from two_stage_actor_design import TwoStagePPOAgent
from two_stage_environment import TwoStageNetworkSchedulerEnvironment

class TwoStagePPOTrainer:
    """ä¸¤é˜¶æ®µPPOè®­ç»ƒå™¨"""
    
    def __init__(self, 
                 num_physical_nodes_range: Tuple[int, int] = (5, 10),
                 max_virtual_nodes_range: Tuple[int, int] = (3, 8),
                 bandwidth_levels: int = 10,
                 # ç‰©ç†èŠ‚ç‚¹èµ„æºèŒƒå›´
                 physical_cpu_range: Tuple[int, int] = (50, 200),
                 physical_memory_range: Tuple[int, int] = (100, 400),
                 physical_bandwidth_range: Tuple[int, int] = (100, 1000),
                 # è™šæ‹ŸèŠ‚ç‚¹èµ„æºèŒƒå›´
                 virtual_cpu_range: Tuple[int, int] = (10, 50),
                 virtual_memory_range: Tuple[int, int] = (20, 100),
                 virtual_bandwidth_range: Tuple[int, int] = (10, 200),
                 # ç½‘ç»œè¿æ¥æ¦‚ç‡
                 physical_connectivity_prob: float = 0.3,
                 virtual_connectivity_prob: float = 0.4,
                 # è®­ç»ƒå‚æ•°
                 lr: float = 3e-4,
                 gamma: float = 0.99,
                 gae_lambda: float = 0.95,
                 clip_ratio: float = 0.2,
                 value_loss_coef: float = 0.5,
                 entropy_coef: float = 0.01,
                 # æ–‡ä»¶ç®¡ç†
                 model_dir: str = "models",
                 stats_dir: str = "stats"):
        
        self.num_physical_nodes_range = num_physical_nodes_range
        self.max_virtual_nodes_range = max_virtual_nodes_range
        self.bandwidth_levels = bandwidth_levels
        
        # èµ„æºèŒƒå›´
        self.physical_cpu_range = physical_cpu_range
        self.physical_memory_range = physical_memory_range
        self.physical_bandwidth_range = physical_bandwidth_range
        self.virtual_cpu_range = virtual_cpu_range
        self.virtual_memory_range = virtual_memory_range
        self.virtual_bandwidth_range = virtual_bandwidth_range
        
        # è¿æ¥æ¦‚ç‡
        self.physical_connectivity_prob = physical_connectivity_prob
        self.virtual_connectivity_prob = virtual_connectivity_prob
        
        # è®­ç»ƒå‚æ•°
        self.lr = lr
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.value_loss_coef = value_loss_coef
        self.entropy_coef = entropy_coef
        
        # æ–‡ä»¶ç®¡ç†
        self.model_dir = model_dir
        self.stats_dir = stats_dir
        
        # åˆ›å»ºç›®å½•
        os.makedirs(self.model_dir, exist_ok=True)
        os.makedirs(self.stats_dir, exist_ok=True)
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'episode_rewards': [],
            'episode_lengths': [],
            'mapping_actor_losses': [],
            'bandwidth_actor_losses': [],
            'critic_losses': [],
            'constraint_violations': [],
            'resource_utilization': [],
            'load_balancing': [],
            'bandwidth_satisfaction': []
        }
        
        # åˆå§‹åŒ–æ™ºèƒ½ä½“å’Œç¯å¢ƒ
        self._initialize_agent_and_env()
    
    def _initialize_agent_and_env(self):
        """åˆå§‹åŒ–æ™ºèƒ½ä½“å’Œç¯å¢ƒ"""
        # éšæœºé€‰æ‹©èŠ‚ç‚¹æ•°é‡
        self.num_physical_nodes = np.random.randint(*self.num_physical_nodes_range)
        self.max_virtual_nodes = np.random.randint(*self.max_virtual_nodes_range)
        
        # åˆ›å»ºç¯å¢ƒ
        self.env = TwoStageNetworkSchedulerEnvironment(
            num_physical_nodes=self.num_physical_nodes,
            max_virtual_nodes=self.max_virtual_nodes,
            bandwidth_levels=self.bandwidth_levels,
            physical_cpu_range=self.physical_cpu_range,
            physical_memory_range=self.physical_memory_range,
            physical_bandwidth_range=self.physical_bandwidth_range,
            virtual_cpu_range=self.virtual_cpu_range,
            virtual_memory_range=self.virtual_memory_range,
            virtual_bandwidth_range=self.virtual_bandwidth_range,
            physical_connectivity_prob=self.physical_connectivity_prob,
            virtual_connectivity_prob=self.virtual_connectivity_prob
        )
        
        # è·å–çŠ¶æ€ç»´åº¦
        state = self.env.reset()
        physical_node_dim = state['physical_features'].size(1)
        virtual_node_dim = state['virtual_features'].size(1)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        self.agent = TwoStagePPOAgent(
            physical_node_dim=physical_node_dim,
            virtual_node_dim=virtual_node_dim,
            num_physical_nodes=self.num_physical_nodes,
            max_virtual_nodes=self.max_virtual_nodes,
            bandwidth_levels=self.bandwidth_levels,
            lr=self.lr,
            gamma=self.gamma,
            gae_lambda=self.gae_lambda,
            clip_ratio=self.clip_ratio,
            value_loss_coef=self.value_loss_coef,
            entropy_coef=self.entropy_coef
        )
        
        print(f"âœ… æ™ºèƒ½ä½“å’Œç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   ç‰©ç†èŠ‚ç‚¹æ•°: {self.num_physical_nodes}")
        print(f"   æœ€å¤§è™šæ‹ŸèŠ‚ç‚¹æ•°: {self.max_virtual_nodes}")
        print(f"   ç‰©ç†èŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {physical_node_dim}")
        print(f"   è™šæ‹ŸèŠ‚ç‚¹ç‰¹å¾ç»´åº¦: {virtual_node_dim}")
    
    def train_episode(self):
        """è®­ç»ƒä¸€ä¸ªepisode"""
        # é‡ç½®ç¯å¢ƒ
        state = self.env.reset()
        
        # è·å–åŠ¨ä½œ
        mapping_action, bandwidth_action, mapping_log_prob, bandwidth_log_prob, value, link_indices = self.agent.select_actions(state)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done, info = self.env.step(mapping_action, bandwidth_action)
        
        # å­˜å‚¨ç»éªŒ
        self.agent.store_transition(
            state, mapping_action, bandwidth_action, 
            reward, value, mapping_log_prob, bandwidth_log_prob, done
        )
        
        # æ›´æ–°ç½‘ç»œ
        self.agent.update()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        episode_stats = {
            'reward': reward,
            'length': 1,  # ä¸¤é˜¶æ®µç¯å¢ƒä¸€æ­¥å®Œæˆ
            'is_valid': info['is_valid'],
            'constraint_violations': len(info['constraint_violations']),
            'mapping_result': info['mapping_result'],
            'bandwidth_result': info['bandwidth_result']
        }
        
        return episode_stats
    
    def train(self, num_episodes: int = 1000, save_interval: int = 100, eval_interval: int = 50):
        """è®­ç»ƒä¸»å¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹ä¸¤é˜¶æ®µPPOè®­ç»ƒ")
        print(f"   æ€»episodes: {num_episodes}")
        print(f"   ä¿å­˜é—´éš”: {save_interval}")
        print(f"   è¯„ä¼°é—´éš”: {eval_interval}")
        print("=" * 60)
        
        start_time = time.time()
        
        for episode in tqdm(range(num_episodes), desc="è®­ç»ƒè¿›åº¦"):
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_stats = self.train_episode()
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            self.training_stats['episode_rewards'].append(episode_stats['reward'])
            self.training_stats['episode_lengths'].append(episode_stats['length'])
            self.training_stats['constraint_violations'].append(episode_stats['constraint_violations'])
            
            # å®šæœŸè¯„ä¼°
            if (episode + 1) % eval_interval == 0:
                self._evaluate_and_log(episode + 1)
            
            # å®šæœŸä¿å­˜
            if (episode + 1) % save_interval == 0:
                self.save_model(episode + 1)
                self.save_training_stats(episode + 1)
        
        # æœ€ç»ˆä¿å­˜
        self.save_model(num_episodes)
        self.save_training_stats(num_episodes)
        
        training_time = time.time() - start_time
        print(f"\nğŸ¯ è®­ç»ƒå®Œæˆï¼")
        print(f"   æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        print(f"   å¹³å‡æ¯episodeæ—¶é—´: {training_time/num_episodes:.3f}ç§’")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_training_curves()
    
    def _evaluate_and_log(self, episode):
        """è¯„ä¼°å¹¶è®°å½•æ—¥å¿—"""
        # è®¡ç®—æœ€è¿‘episodesçš„å¹³å‡å¥–åŠ±
        recent_rewards = self.training_stats['episode_rewards'][-50:]
        avg_reward = np.mean(recent_rewards)
        
        # è®¡ç®—çº¦æŸè¿åç‡
        recent_violations = self.training_stats['constraint_violations'][-50:]
        violation_rate = np.mean([1 if v > 0 else 0 for v in recent_violations])
        
        print(f"Episode {episode:4d} | å¹³å‡å¥–åŠ±: {avg_reward:6.3f} | çº¦æŸè¿åç‡: {violation_rate:.2%}")
    
    def save_model(self, episode):
        """ä¿å­˜æ¨¡å‹"""
        model_path = os.path.join(self.model_dir, f"two_stage_ppo_model_episode_{episode}.pth")
        
        torch.save({
            'episode': episode,
            'mapping_actor_state_dict': self.agent.mapping_actor.state_dict(),
            'bandwidth_actor_state_dict': self.agent.bandwidth_actor.state_dict(),
            'critic_state_dict': self.agent.critic.state_dict(),
            'mapping_optimizer_state_dict': self.agent.mapping_optimizer.state_dict(),
            'bandwidth_optimizer_state_dict': self.agent.bandwidth_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.agent.critic_optimizer.state_dict(),
            'training_stats': self.training_stats,
            'env_config': {
                'num_physical_nodes': self.num_physical_nodes,
                'max_virtual_nodes': self.max_virtual_nodes,
                'bandwidth_levels': self.bandwidth_levels,
                'physical_cpu_range': self.physical_cpu_range,
                'physical_memory_range': self.physical_memory_range,
                'physical_bandwidth_range': self.physical_bandwidth_range,
                'virtual_cpu_range': self.virtual_cpu_range,
                'virtual_memory_range': self.virtual_memory_range,
                'virtual_bandwidth_range': self.virtual_bandwidth_range,
                'physical_connectivity_prob': self.physical_connectivity_prob,
                'virtual_connectivity_prob': self.virtual_connectivity_prob
            }
        }, model_path)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def load_model(self, episode):
        """åŠ è½½æ¨¡å‹"""
        model_path = os.path.join(self.model_dir, f"two_stage_ppo_model_episode_{episode}.pth")
        
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
        
        checkpoint = torch.load(model_path, map_location=self.agent.device)
        
        self.agent.mapping_actor.load_state_dict(checkpoint['mapping_actor_state_dict'])
        self.agent.bandwidth_actor.load_state_dict(checkpoint['bandwidth_actor_state_dict'])
        self.agent.critic.load_state_dict(checkpoint['critic_state_dict'])
        self.agent.mapping_optimizer.load_state_dict(checkpoint['mapping_optimizer_state_dict'])
        self.agent.bandwidth_optimizer.load_state_dict(checkpoint['bandwidth_optimizer_state_dict'])
        self.agent.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
        
        self.training_stats = checkpoint['training_stats']
        
        print(f"ğŸ“‚ æ¨¡å‹å·²åŠ è½½: {model_path}")
        return True
    
    def save_training_stats(self, episode):
        """ä¿å­˜è®­ç»ƒç»Ÿè®¡"""
        stats_path = os.path.join(self.stats_dir, f"two_stage_training_stats_episode_{episode}.json")
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_stats = {}
        for key, value in self.training_stats.items():
            if isinstance(value, list):
                serializable_stats[key] = [float(v) if isinstance(v, (int, float)) else v for v in value]
            else:
                serializable_stats[key] = value
        
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_stats, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_path}")
    
    def _plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # å¥–åŠ±æ›²çº¿
            axes[0, 0].plot(self.training_stats['episode_rewards'])
            axes[0, 0].set_title('Episode Rewards')
            axes[0, 0].set_xlabel('Episode')
            axes[0, 0].set_ylabel('Reward')
            axes[0, 0].grid(True)
            
            # çº¦æŸè¿åç‡
            violation_rates = [1 if v > 0 else 0 for v in self.training_stats['constraint_violations']]
            window_size = 50
            moving_avg = []
            for i in range(len(violation_rates)):
                start = max(0, i - window_size + 1)
                moving_avg.append(np.mean(violation_rates[start:i+1]))
            
            axes[0, 1].plot(moving_avg)
            axes[0, 1].set_title('Constraint Violation Rate (Moving Average)')
            axes[0, 1].set_xlabel('Episode')
            axes[0, 1].set_ylabel('Violation Rate')
            axes[0, 1].grid(True)
            
            # å¹³å‡å¥–åŠ±ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
            reward_moving_avg = []
            for i in range(len(self.training_stats['episode_rewards'])):
                start = max(0, i - window_size + 1)
                reward_moving_avg.append(np.mean(self.training_stats['episode_rewards'][start:i+1]))
            
            axes[1, 0].plot(reward_moving_avg)
            axes[1, 0].set_title('Average Reward (Moving Average)')
            axes[1, 0].set_xlabel('Episode')
            axes[1, 0].set_ylabel('Average Reward')
            axes[1, 0].grid(True)
            
            # çº¦æŸè¿åæ•°é‡
            axes[1, 1].plot(self.training_stats['constraint_violations'])
            axes[1, 1].set_title('Constraint Violations')
            axes[1, 1].set_xlabel('Episode')
            axes[1, 1].set_ylabel('Number of Violations')
            axes[1, 1].grid(True)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            plot_path = os.path.join(self.stats_dir, 'training_curves.png')
            plt.savefig(plot_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“ˆ è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {plot_path}")
            
            plt.close()
            
        except Exception as e:
            print(f"âš ï¸ ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")
    
    def test_agent(self, num_test_episodes: int = 10):
        """æµ‹è¯•æ™ºèƒ½ä½“"""
        print(f"\nğŸ§ª æµ‹è¯•æ™ºèƒ½ä½“ ({num_test_episodes} episodes)")
        print("=" * 60)
        
        test_stats = {
            'rewards': [],
            'constraint_violations': [],
            'valid_actions': 0,
            'total_actions': 0
        }
        
        for episode in range(num_test_episodes):
            # é‡ç½®ç¯å¢ƒ
            state = self.env.reset()
            
            # è·å–åŠ¨ä½œ
            mapping_action, bandwidth_action, _, _, _, _ = self.agent.select_actions(state)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            _, reward, _, info = self.env.step(mapping_action, bandwidth_action)
            
            # è®°å½•ç»Ÿè®¡
            test_stats['rewards'].append(reward)
            test_stats['constraint_violations'].append(len(info['constraint_violations']))
            test_stats['total_actions'] += 1
            
            if info['is_valid']:
                test_stats['valid_actions'] += 1
            
            # æ‰“å°ç»“æœ
            print(f"Episode {episode + 1:2d} | å¥–åŠ±: {reward:6.3f} | æœ‰æ•ˆ: {info['is_valid']} | è¿å: {len(info['constraint_violations'])}")
        
        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        avg_reward = np.mean(test_stats['rewards'])
        avg_violations = np.mean(test_stats['constraint_violations'])
        valid_rate = test_stats['valid_actions'] / test_stats['total_actions']
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.3f}")
        print(f"   å¹³å‡çº¦æŸè¿å: {avg_violations:.2f}")
        print(f"   æœ‰æ•ˆåŠ¨ä½œç‡: {valid_rate:.2%}")
        
        return test_stats

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¸¤é˜¶æ®µPPOç½‘ç»œè°ƒåº¦å™¨è®­ç»ƒ")
    print("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = TwoStagePPOTrainer(
        num_physical_nodes_range=(5, 8),
        max_virtual_nodes_range=(3, 6),
        bandwidth_levels=10,
        physical_cpu_range=(50.0, 200.0),
        physical_memory_range=(100.0, 400.0),
        physical_bandwidth_range=(100.0, 1000.0),
        virtual_cpu_range=(10.0, 50.0),
        virtual_memory_range=(20.0, 100.0),
        virtual_bandwidth_range=(10.0, 200.0),
        physical_connectivity_prob=0.3,
        virtual_connectivity_prob=0.4,
        lr=3e-4,
        gamma=0.99,
        gae_lambda=0.95,
        clip_ratio=0.2,
        value_loss_coef=0.5,
        entropy_coef=0.01
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(num_episodes=500, save_interval=100, eval_interval=50)
    
    # æµ‹è¯•æ™ºèƒ½ä½“
    trainer.test_agent(num_test_episodes=10)
    
    print(f"\nğŸ‰ è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 